{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 강의 - softmax classification과 cnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples.tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-29a98e9e82d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 받아오자\n",
    "#one hot data는 하나의 값만 True이고 나머지는 모두 False인 인코딩을 말한다.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax classification-basic NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우 노드를 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tf.float는 실수,정수등의 데이타 타입의 종류\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,784],name='x')\n",
    "y=tf.placeholder(tf.float32,[None,10],name='y')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설함수 H(x)=Wx+b를 만들어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.name_scope는 name scope 아래에 기능을 정의\n",
    "#variable을 정의\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1=tf.Variable(tf.random_normal([784,28]),name='weight1')\n",
    "    b1=tf.Variable(tf.random_normal([28]),name='bias1')\n",
    "    layer1=tf.sigmoid(tf.matmul(x,W1)+b1)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2=tf.Variable(tf.random_normal([28,10]),name='weight2')\n",
    "    b2=tf.Variable(tf.random_normal([10]),name='bias2')\n",
    "    \n",
    "    #tf.matmul은 3차원 텐서 사이에서 행렬 곱을 하는 코드\n",
    "    \n",
    "    logits=tf.matmul(layer1,W2)+b2\n",
    "    hypo=tf.nn.softmax(logits)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost함수를 만들어 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=y)\n",
    "#cross entropy를 손실 함수(cost function)로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost를 줄이는 optimizer로 우리가 잘 아는 경사하강법(gradient descent)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GradientDescentOptimizer는 경사하강법을 이용하는 알고리즘, cost를 줄여주는 작업에 들어감\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 우리가 만든 가설함수의 정확성을 확인하기 위한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction=tf.argmax(hypo,axis=1)\n",
    "\n",
    "is_correct=tf.equal(prediction,tf.argmax(y,1))\n",
    "  \n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 지금까지는 가설함수와 층을 만들고, 코스트를 줄이는 방식(여기서는 경사하강법)을 정하는 등 텐서플로우 상에서 'graph를 그린 것'입니다. \n",
    "### 이제부터 할 것은 실제로 그 안에서 가설함수의 코스트를 줄이는 방향으로 학습을 진행하라는 코드를 볼 것입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-880f29f295d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m#이는 sess=tf.Session()과 같은 의미로 계산 과정을 시작하자는 시동을 거는 것입니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#parameters로 iter_epoch는 전체 학습 반복 횟수, batch_size는 한번에 읽어드려서 학습시키는 양을 의미합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#batch_size는 왜 필요할까요?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    " \n",
    "with tf.Session() as sess:\n",
    "    #텐서플로우의 오퍼레이션, 즉 노드를 실행하기 위한 클래스 \n",
    "\n",
    "    iter_epoch=15\n",
    "    batch_size=100\n",
    "    \n",
    "    #sess.run은 오퍼레이션 객체(tf.Operation)를 실행하거나 텐서 객체(tf.Tensor)의 값을 구하기 위한 주요 메커니즘\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(iter_epoch):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다\n",
    "        # Loop over all batches \n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "            # batch_size만큼씩 읽어라\n",
    "            \n",
    "            \n",
    "            c,s,_=sess.run([cost,accuracy,optimizer],feed_dict={x:batch_x,y:batch_y})\n",
    "            \n",
    "            avg_cost+=c/total_batch\n",
    "            \n",
    "            \n",
    "        print(\"Accuracy\",accuracy.eval(session=sess,feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "        \n",
    "        r = random.randint(0, mnist.test.num_examples - 1)\n",
    "        print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "        print(\"Prediction: \", sess.run(\n",
    "            tf.argmax(logits, 1), feed_dict={x: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "        plt.imshow(mnist.test.images[r:r + 1].\n",
    "                   reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'set_random_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc93e5d9bd14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m777\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reproducibility를 위해 지정해둡니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  \n",
    "# random seed 생성\n",
    "\n",
    "# 파라미터 \n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1d520452cd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dropout (keep_prob) rate  0.7~0.5 가 train시 권장되고 test 시에는 1을 사용해야합니다. 이 부분은 뒤에서 ppt와 함께 다시 설명해드리겠습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "#placeholder:변수의 타입을 미리 설정해놓고 필요한 변수를 나중에 받아서 실행하는 것을 의미\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 기본 노드들을 만들어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (흑백 사진이기에, 컬러였으면 RGB로 28*28*3 이었을 것입니다)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn 필터들과 다층 layer를 만들어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob) \n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "'''\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                    1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "\n",
    "'''\n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn 끝 부분에서 위에서 배웠던 softmax을 연결하여 학습시킵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"add_1:0\", shape=(?, 10), dtype=float32)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "'''\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "'''\n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-350b1f9f0293>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "#AdamOptimizer는 왜 사용할까요 ?\n",
    "#-> Adam method의 의 주요 장점은 stepsize가 gradient의 rescaling에 영향 받지 않는다는 것\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 본격적인 학습을 시작합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.523530539\n",
      "Epoch: 0002 cost = 0.097408769\n",
      "Epoch: 0003 cost = 0.072850064\n",
      "Epoch: 0004 cost = 0.057451008\n",
      "Epoch: 0005 cost = 0.050928287\n",
      "Epoch: 0006 cost = 0.046455198\n",
      "Epoch: 0007 cost = 0.039119159\n",
      "Epoch: 0008 cost = 0.038678807\n",
      "Epoch: 0009 cost = 0.035540133\n",
      "Epoch: 0010 cost = 0.034494487\n",
      "Epoch: 0011 cost = 0.031486203\n",
      "Epoch: 0012 cost = 0.028180883\n",
      "Epoch: 0013 cost = 0.027664424\n",
      "Epoch: 0014 cost = 0.026479798\n",
      "Epoch: 0015 cost = 0.026021254\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "#sess = tf.Session()와  위에서 쓴 with tf.Session() as sess는 동일한 의미 \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9934\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf167FwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#correct prediction 하기 위해 각 tr.argmax를 비교하여 boolean 값을 반환\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "#random.radint : 2개의 숫자 사이의 랜덤 정수를 리턴\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
